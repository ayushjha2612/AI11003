\documentclass{beamer}
\usepackage{listings}
\lstset{
%language=C,
frame=single, 
breaklines=true,
columns=fullflexible
}
\usepackage{blkarray}
\usepackage{subcaption}
\usepackage{url}
\usepackage{tikz}
\usepackage{tkz-euclide} % loads  TikZ and tkz-base
%\usetkzobj{all}
\usetikzlibrary{calc,math}
\usepackage{float}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\renewcommand{\vec}[1]{\mathbf{#1}}
\usepackage[export]{adjustbox}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{bm}
\hypersetup{
    colorlinks = true,
    linkbordercolor = {white},
    linkcolor={red},
    citecolor={green},
    filecolor={blue},
	menucolor={red},
	runcolor={cyan},
	urlcolor={blue},
	breaklinks=true
}
\usetikzlibrary{automata, positioning}
\usetheme{Boadilla}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\mbf}{\mathbf}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\providecommand{\abs}[1]{\vert#1\vert}

\title{CSIR-UGC NET-June 2013-Problem(68)}
\author{Ayush Jha}
\date{CS20BTECH11006}
\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Question}
\begin{block}{CSIR-UGC NET-June 2013-Problem(68)}
Let $ X_1, \cdots , X_n $ be independent and identically distributed random variables with probability density function
\begin{align*}
    f(x) = \frac{1}{2} \lambda^3x^2e^{-\lambda x} ; x>0 ; \lambda > 0
\end{align*}
Then which of the following statements are true?
\begin{enumerate}
    \item $\dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} $ is an unbiased estimator of $ \lambda$
    \item $\dfrac{3n}{\sum_{i=1}^{n} X_i } $ is an unbiased estimator of $ \lambda$ \\
    \item $\dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} $ is a consistent estimator of $ \lambda$
    \item $\dfrac{3n}{\sum_{i=1}^{n} X_i } $ is a consistent estimator of $ \lambda$
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Definitions}
\begin{block}{What is estimator?}
An estimator is a statistic that estimates some fact about the population. For example, the sample mean ($\bar{X}$) is an estimator for the population mean, $ \mu$.
The quantity that is being estimated is called the \textbf{estimand.} 
\end{block}
\begin{block}{Bias of estimator}
Let $ \Theta = h(X_1,X_2, \cdots, X_n) $  be a point estimator for $ \theta$. The \textbf{bias} of the estimator $ \Theta $ is defined by 
\begin{align}
    B(\Theta ) = E[\Theta ] - \theta
\end{align}
where $ E[\Theta ]$ is the expectation value of the estimator $ \Theta $ and $ \theta$ is the estimand. 
\end{block}
\end{frame}

\begin{frame}
\frametitle{Definitions Contd.}
\begin{block}{Unbiased estimator}
 Let $\Theta = h(X_1,X_2, \cdots , X_n) $ be a point estimator for a parameter $ \theta $. We say that $ \Theta $ is an \textbf{unbiased estimator} of $ \theta $ if
    \begin{align}
       B(\Theta )= 0 \text{, for all possible values of $\theta$.}
    \end{align}
\end{block}
\begin{block}{Consistent estimator}
 Let $ \Theta_1,\Theta_2, \cdots, \Theta_n , \cdots, $  be a sequence of point estimators of $ \theta $. We say that $ \Theta_n $ is a \textbf{consistent} estimator of $ \theta $, if 
\begin{align}
    \lim_{n\to\infty} \Pr\brak{| \Theta_n - \theta | \ge \epsilon} = 0 \text{ ,for all $ \epsilon > 0$.}
\end{align}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Definitions Contd.}
\begin{block}{Mean Squared Error (MSE)}
 The \textbf{mean squared error (MSE)} of a point estimator $ \Theta $, shown by $ MSE(\Theta) $, is defined as
\begin{align}
    MSE(\Theta ) &= E[(\Theta - \theta)^2] \\
    &= Var(\Theta) + B(\Theta)^2
\end{align}
where $ B(\Theta ) $ is the bias of $ \Theta $. 
\end{block}
\begin{block}{Theorem}
  Let $ \Theta_1,\Theta_2 , \cdots$ be a sequence of point estimators of $ \theta $. If
\begin{align}
     \lim_{n\to\infty} MSE( \Theta_n) = 0,
\end{align}
then $ \Theta_n $ is a consistent estimator of $ \theta$.
\end{block}
\end{frame}
\begin{frame}
\frametitle{Solution: Option 1}
 Now here we have our estimator $ \Theta$ and estimand $ \theta $ as,
 \begin{align}
     \Theta = \dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} \text{  and  }
     \theta = \lambda
 \end{align}
 To check if this is an unbiased estimator or not the bias needs to be calculated.
 \begin{align}
    B(\Theta ) = E[\Theta ] - \theta
\end{align}
\end{frame}

\begin{frame}
\frametitle{Option 1 Contd.}
The expectation value of the estimator is given by, 
\begin{align}
    E[\Theta ] &= E  \left[   \dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i}  \right] \\
    & = \dfrac{2}{n} \sum_{i=1}^{n} E  \left[ \dfrac{1}{X_i}  \right] \\
    & =  \dfrac{2}{n} \sum_{i=1}^{n} \int_{-\infty}^{\infty} \dfrac{1}{x} f(x)\,dx \\
    \label{eq1}
    & = \dfrac{2n}{n} \int_{0}^{\infty} \dfrac{1}{x} \dfrac{1}{2} \lambda^3x^2e^{-\lambda x}\,dx \\
    & = \lambda^3  \int_{0}^{\infty}  x e^{-\lambda x}\,dx \\
    &= \lambda
\end{align}
\end{frame}

\begin{frame}
\frametitle{Option 1 Contd.}
From above calculations we can say,
\begin{align}
    E  \left[ \dfrac{1}{X}  \right] = \dfrac{\lambda}{2}
    \label{eq}
\end{align}
So the bias of estimator is given by,
\begin{align}
    B(\Theta) &= E[\Theta] - \theta  \\
    &= \lambda - \lambda = 0
\end{align}
Therefore $\dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} $ is an unbiased estimator of $ \lambda$ .\\
\textbf{Option 1 is correct.} \\
\end{frame}

\begin{frame}
\frametitle{Option 2}
Here in this option, we have our estimator $ \Theta$ and estimand $ \theta $ as,
 \begin{align}
     \Theta = \dfrac{3n}{\sum_{i=1}^{n} X_i } \text{  and  }
     \theta = \lambda
 \end{align}
 The expectation value of the estimator is given by, 
\begin{align}
    E[\Theta ] &= E  \left[   \dfrac{3n}{\sum_{i=1}^{n} X_i }  \right] \\
    & = \dfrac{3n}{\sum_{i=1}^{n}}  E  \left[ \dfrac{1}{X_i}  \right] 
\end{align}
\end{frame}
\begin{frame}
   \frametitle{Option 2 Contd.} 
The value of $  E  \left[ \dfrac{1}{X_i}  \right]  $ can be obtained from \eqref{eq} as so we have,
\begin{align}
     E[\Theta ] &= \dfrac{3n}{\sum_{i=1}^{n}} \dfrac{\lambda}{2} \\
     &= \dfrac{3n}{n} \dfrac{\lambda}{2}
     = \dfrac{3\lambda}{2}
\end{align}
So the bias of estimator is given by,
\begin{align}
    B(\Theta) &= E[\Theta] - \theta  \\
    &= \dfrac{3\lambda}{2}- \lambda  \neq 0
    \label{eqbias2}
\end{align}
Therefore $ \dfrac{3n}{\sum_{i=1}^{n} X_i } $ is not an unbiased estimator of $ \lambda$ \\
\textbf{Option 2 is not correct.}
\end{frame}

 \begin{frame}
 \frametitle{Option 3}
    Now here we have our estimator $ \Theta$ and estimand $ \theta $ as,
 \begin{align}
     \Theta = \dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} \text{  and  }
     \theta = \lambda
 \end{align}  
 Now the variance of $ \Theta$ is calculated as
\begin{align}
    Var(\Theta) &= Var\brak{\dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} } \\
    & = \dfrac{4}{n^2} \sum_{i=1}^{n} Var\brak{\dfrac{1}{X_i}} \\
    \label{eq3}
    & = \dfrac{4n}{n^2} \brak{E  \left[ {\dfrac{1}{X_i}}^2  \right] - {E  \left[ {\dfrac{1}{X_i}}  \right]}^2 } \\
    & = \dfrac{4}{n} \brak{ \int_{-\infty}^{\infty} \dfrac{1}{x^2} f(x)\,dx  - \brak{\dfrac{\lambda}{2}}^2 } 
\end{align}
\end{frame}

\begin{frame}
 \frametitle{Option 3 Contd.}
\begin{align}
     Var(\Theta)  & =  \dfrac{4}{n} \brak{ \int_{0}^{\infty} \dfrac{1}{x^2}  \dfrac{1}{2} \lambda^3x^2e^{-\lambda x} \,dx  - {\dfrac{\lambda^2}{4}} } \\
    &= \dfrac{\lambda^2}{n}
\end{align}
The bias of $ \Theta $ from option 1 is given as $B(\Theta) = 0$
So we have,
\begin{align}
    MSE(\Theta_n) &= Var(\Theta) + B(\Theta)^2 \\
    &= \dfrac{\lambda^2}{n}
\end{align}
\begin{align}
     \lim_{n\to\infty} MSE( \Theta_n) &=    \lim_{n\to\infty} \dfrac{\lambda^2}{n} \\
      &= 0
\end{align}
Therefore, $\dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} $ is a consistent estimator of $ \lambda$. \\
\textbf{Option 3 is correct.}
\end{frame}
 
 \begin{frame}
 \frametitle{Option 4}
Now here we have our estimator $ \Theta$ and estimand $ \theta $ as,
 \begin{align}
     \Theta = \dfrac{3n}{\sum_{i=1}^{n} X_i } \text{  and  }
     \theta = \lambda
 \end{align}
Now the variance of $ \Theta$ is calculated as
\begin{align}
    Var(\Theta) &= Var\brak{\dfrac{3n}{\sum_{i=1}^{n} X_i } } \\
    & = \dfrac{9n^2}{ \sum_{i=1}^{n}} Var\brak{\dfrac{1}{X_i}} \\
\end{align}
Now the value of $ Var\brak{\dfrac{1}{X_i}} $ from \eqref{eq3} is substituted, we have
 \end{frame}

 \begin{frame}
 \frametitle{Option 4 Contd.}
\begin{align}
     Var(\Theta) &= \dfrac{9n^2}{ \sum_{i=1}^{n}} \dfrac{\lambda^2}{4} \\
     & = \dfrac{9n^2 \lambda^2}{4n } \\
     & = \dfrac{9n \lambda^2}{4 }
\end{align}
The bias of $ \Theta $ from option 2 is given as $ B(\Theta) = \dfrac{\lambda}{2} $
So we have,
\begin{align}
    MSE(\Theta_n) &= Var(\Theta) + B(\Theta)^2 \\
    &= \dfrac{9n \lambda^2}{4 } + \brak{\dfrac{\lambda}{2}}^2 \\
    & = \dfrac{ \lambda^2}{4 } (9n+1)
\end{align}
 \end{frame}

\begin{frame}
 \frametitle{Option 4 Contd.}
Now,
\begin{align}
     \lim_{n\to\infty} MSE( \Theta_n) &=    \lim_{n\to\infty} \dfrac{ \lambda^2}{4 } (9n+1) \\
\end{align}
Clearly as n grows larger $ 9n+1$ also grows larger, so
\begin{align}
     \lim_{n\to\infty} MSE( \Theta_n) \neq 0   
\end{align}
Therefore, $\dfrac{3n}{\sum_{i=1}^{n} X_i} $ is not a consistent estimator of $ \lambda$. \\
Option 4 is not correct. \\
\textbf{Therefore option 1 and option 3 are correct.}
\end{frame}
\end{document}