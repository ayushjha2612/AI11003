\documentclass[journal,12pt,twocolumn]{IEEEtran}

\usepackage{setspace}
\usepackage{gensymb}
\singlespacing
\usepackage[cmex10]{amsmath}

\usepackage{amsthm}

\usepackage{mathrsfs}
\usepackage{txfonts}
\usepackage{stfloats}
\usepackage{bm}
\usepackage{cite}
\usepackage{cases}
\usepackage{subfig}

\usepackage{longtable}
\usepackage{multirow}

\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{steinmetz}
\usepackage{tikz}
\usepackage{circuitikz}
\usepackage{verbatim}
\usepackage{tfrupee}
\usepackage[breaklinks=true]{hyperref}
\usepackage{graphicx}
\usepackage{tkz-euclide}

\usetikzlibrary{calc,math}
\usepackage{listings}
    \usepackage{color}                                            %%
    \usepackage{array}                                            %%
    \usepackage{longtable}                                        %%
    \usepackage{calc}                                             %%
    \usepackage{multirow}                                         %%
    \usepackage{hhline}                                           %%
    \usepackage{ifthen}                                           %%
    \usepackage{lscape}     
\usepackage{multicol}
\usepackage{chngcntr}

\DeclareMathOperator*{\Res}{Res}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}

\renewcommand\thesectiondis{\arabic{section}}
\renewcommand\thesubsectiondis{\thesectiondis.\arabic{subsection}}
\renewcommand\thesubsubsectiondis{\thesubsectiondis.\arabic{subsubsection}}


\hyphenation{op-tical net-works semi-conduc-tor}
\def\inputGnumericTable{}                                 %%

\lstset{
%language=C,
frame=single, 
breaklines=true,
columns=fullflexible
}
\begin{document}
\newcommand{\Int}{\int\limits}
\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}
\bibliographystyle{IEEEtran}
\raggedbottom
\setlength{\parindent}{0pt}
\providecommand{\mbf}{\mathbf}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\providecommand{\abs}[1]{\vert#1\vert}
\providecommand{\res}[1]{\Res\displaylimits_{#1}} 
\providecommand{\norm}[1]{\lVert#1\rVert}
%\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\mtx}[1]{\mathbf{#1}}
\providecommand{\mean}[1]{E[ #1 ]}
\providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
%\providecommand{\hilbert}{\overset{\mathcal{H}}{ \rightleftharpoons}}
\providecommand{\system}{\overset{\mathcal{H}}{ \longleftrightarrow}}
	%\newcommand{\solution}[2]{\textbf{Solution:}{#1}}
\newcommand{\solution}{\noindent \textbf{Solution: }}
\newcommand{\cosec}{\,\text{cosec}\,}
\providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
\newcommand{\myvec}[1]{\ensuremath{\begin{pmatrix}#1\end{pmatrix}}}
\newcommand{\mydet}[1]{\ensuremath{\begin{vmatrix}#1\end{vmatrix}}}
\numberwithin{equation}{subsection}
\makeatletter
\@addtoreset{figure}{problem}
\makeatother
\let\StandardTheFigure\thefigure
\let\vec\mathbf
\renewcommand{\thefigure}{\theproblem}
\def\putbox#1#2#3{\makebox[0in][l]{\makebox[#1][l]{}\raisebox{\baselineskip}[0in][0in]{\raisebox{#2}[0in][0in]{#3}}}}
     \def\rightbox#1{\makebox[0in][r]{#1}}
     \def\centbox#1{\makebox[0in]{#1}}
     \def\topbox#1{\raisebox{-\baselineskip}[0in][0in]{#1}}
     \def\midbox#1{\raisebox{-0.5\baselineskip}[0in][0in]{#1}}
\vspace{3cm}
\title{AI1103-Assignment 8}
\author{Name : Ayush Jha \\ Roll Number: CS20BTECH11006}
\maketitle
\newpage
\bigskip
\renewcommand{\thefigure}{\theenumi}
\renewcommand{\thetable}{\theenumi}
Download all python codes from 
\begin{lstlisting}
https://github.com/ayushjha2612/AI11003/tree/main/Assignment8/Codes
\end{lstlisting}
%
and latex-tikz codes from 
%
\begin{lstlisting}
https://github.com/ayushjha2612/AI11003/tree/main/Assignment8
\end{lstlisting}

\section*{CSIR UGC NET EXAM (June 2013) Q. 68}
Let $ X_1, \cdots , X_n $ be independent and identically distributed random variables with probability density function
\begin{align}
    f(x) = \frac{1}{2} \lambda^3x^2e^{-\lambda x} ; x>0 ; \lambda > 0
    \label{pdf}
\end{align}
Then which of the following statements are true?
\begin{enumerate}
    \item $\dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} $ is an unbiased estimator of $ \lambda$
    \item $\dfrac{3n}{\sum_{i=1}^{n} X_i } $ is an unbiased estimator of $ \lambda$ \\
    \item $\dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} $ is a consistent estimator of $ \lambda$
    \item $\dfrac{3n}{\sum_{i=1}^{n} X_i } $ is a consistent estimator of $ \lambda$
\end{enumerate}

\section*{Solution}
\begin{definition}
An \textbf{estimator} is a statistic that estimates some fact about the population.
The quantity that is being estimated is called the \textbf{estimand.} 
\end{definition}
\begin{definition}
Let $ \Theta = h(X_1,X_2, \cdots, X_n) $  be a point estimator for $ \theta$. The \textbf{bias} of the estimator $ \Theta $ is defined by 
\begin{align}
    B(\Theta ) = E[\Theta ] - \theta
\end{align}
where $ E[\Theta ]$ is the expectation value of the estimator $ \Theta $ and $ \theta$ is the estimand.
\end{definition}

    \begin{definition}
    Let $\Theta = h(X_1,X_2, \cdots , X_n) $ be a point estimator for a parameter $ \theta $. We say that $ \Theta $ is an \textbf{unbiased estimator} of $ \theta $ if
    \begin{align}
       B(\Theta )= 0 \text{, for all possible values of $\theta$.}
    \end{align}
    \end{definition} 

\begin{definition}
Let $ \Theta_1,\Theta_2, \cdots, \Theta_n , \cdots, $  be a sequence of point estimators of $ \theta $. We say that $ \Theta_n $ is a \textbf{consistent} estimator of $ \theta $, if 
\begin{align}
    \lim_{n\to\infty} \Pr\brak{| \Theta_n - \theta | \ge \epsilon} = 0 \text{ ,for all $ \epsilon > 0$.}
\end{align}
\end{definition}
\begin{definition}
The \textbf{mean squared error (MSE)} of a point estimator $ \Theta $, shown by $ MSE(\Theta) $, is defined as
\begin{align}
    MSE(\Theta ) &= E[(\Theta - \theta)^2] \\
    &= Var(\Theta) + B(\Theta)^2
\end{align}
where $ B(\Theta ) $ is the bias of $ \Theta $. 
\end{definition}    

\begin{theorem}
 Let $ \Theta_1,\Theta_2 , \cdots$ be a sequence of point estimators of $ \theta $. If
\begin{align}
     \lim_{n\to\infty} MSE( \Theta_n) = 0,
\end{align}
then $ \Theta_n $ is a consistent estimator of $ \theta$.
\end{theorem}

\begin{definition}
The \textbf{moment generating function (MGF)} of a random variable $X$ is a function $ M_X(s) $ defined as
\begin{align}
    M_X(s) = E[e^{sX}].
\end{align}
\end{definition}
\begin{lemma}
\label{ibp}
A well known result of Integratin by Parts is 
\begin{align}
    \int_{0}^{\infty} x^a e^{-bx}\,dx = \dfrac{\Gamma(a+1)}{b^{a+1}}
    \label{eqidentity}
\end{align}
where $ \Gamma(a) = (a-1)!$ for $ a \in Z$
\end{lemma}
\begin{theorem}
\label{gamma}
 The Moment generating function of a gamma distribution with PDF as,
 \begin{align}
f_{X}(x)  = 
\begin{cases}
\dfrac{\lambda^{\alpha}x^{\alpha-1}e^{-\lambda x}}{\Gamma(\alpha)} &  x > 0
\\
0 & otherwise
\end{cases}
\end{align}
where $ \Gamma(\alpha) = \dfrac{1}{(\alpha-1)!}$
is given by,
\begin{align}
    M_X(s) = \brak{\dfrac{\lambda}{\lambda-s}}^{\alpha}
\end{align}
\end{theorem}
\begin{proof}
\begin{align}
    M_X(s) &= E[e^{sX}] \\
    &=  \int_{-\infty}^{\infty} e^{sx} f(x)\,dx \\
    &=    \int_{0}^{\infty} e^{sx} \dfrac{1}{\Gamma(\alpha)} \lambda^\alpha x^{\alpha-1}e^{-\lambda x}\,dx \\
    &= \dfrac{\lambda^\alpha}{\Gamma(\alpha)} \int_{0}^{\infty} x^{\alpha-1} e^{-(\lambda - s) x}\,dx 
\end{align}

From lemma \ref{ibp} we have,
\begin{align}
    \int_{0}^{\infty} x^a e^{-bx}\,dx = \dfrac{\Gamma(a+1)}{b^{a+1}}
    \label{eqidentity}
\end{align}
Making this substitution we get,
\begin{align}
  \int_{0}^{\infty} x^{\alpha-1} e^{-(\lambda - s) x}\,dx    &= \dfrac{\Gamma(\alpha)}{(\lambda - s)^\alpha}
\end{align}
Therefore,
\begin{align}
     M_X(s) &= \dfrac{\lambda^\alpha}{\Gamma(\alpha)} \times \dfrac{\Gamma(\alpha)}{(\lambda-s)^\alpha} \\
     &= \brak{\dfrac{\lambda}{\lambda-s}}^\alpha
\end{align}
\end{proof}

\begin{theorem}
\label{mgf}
 The MGF (if it exists) uniquely determines the distribution. That is, if two random variables have the same MGF, then they must have the same distribution. 
\end{theorem}
\begin{lemma}
\label{var}
If $X_{i}$ for $i = 1,2,\dots,n$ are independent and identically distributed random variables (i.i.ds), then we have the following property, 
   \begin{align}
    Var\brak{a\sum_{i=1}^{n}g(X_{i})}=a^2 \sum_{i=1}^{n}Var(g(X_{i}))\label{basic}
    \end{align}
\end{lemma}
\begin{lemma}
\label{gp}
One of the property of gamma distributions states that,
\begin{align}
    \int_{0}^{\infty}  \lambda^{\alpha}t^{\alpha-1}e^{-\lambda t}\,dt \\ 
    = \dfrac{1}{\Gamma(\alpha)}
\end{align}
\end{lemma}
\begin{lemma}
\label{expected}
The expected value of $ \dfrac{1}{\Bar{X}} $ with distribution of X from equation \eqref{pdf} is
\begin{align}
        E \left[  \dfrac{1}{\Bar{X}}  \right] &= \dfrac{n\lambda}{3n-1}
\end{align}
\end{lemma}
\begin{proof}
Let r.v. $T $ be,
\begin{align}
    T = \sum_{i=1}^{n} X_i \sim \Gamma(3n, \lambda)
\end{align}
with pdf,
\begin{align}
    f_T(t)= \dfrac{\lambda^{3n} t^{3n-1}e^{-\lambda t}}{\Gamma(3n)} , t>0
\end{align}
Using, $ \dfrac{1}{\Bar{X}} = \dfrac{n}{T}$
\begin{align}
   & E \left[   \dfrac{1}{\Bar{X}}  \right] =  \int_{0}^{\infty} \dfrac{n}{t} \dfrac{1}{\Gamma(3n)} \lambda^{3n}t^{3n-1}e^{-\lambda t}\,dt \\
    &= \dfrac{n\lambda}{(3n-1)} \int_{0}^{\infty}  \dfrac{1}{\Gamma(3n-1)} \lambda^{3n-1}t^{3n-2}e^{-\lambda t}\,dt 
    \label{expect}
\end{align}
Using lemma \ref{gp} we have,
\begin{align}
    \int_{0}^{\infty}  \dfrac{1}{\Gamma(3n-1)} \lambda^{3n-1}t^{3n-2}e^{-\lambda t}\,dt = 1
    \label{1}
\end{align}
Using equations \eqref{1} and \eqref{expect} we have,
\begin{align}
    E \left[  \dfrac{1}{\Bar{X}}  \right] &= \dfrac{n\lambda}{3n-1}
\end{align}
\end{proof}
\begin{lemma}
\label{varxbar}
The variance of $ \dfrac{1}{\Bar{X}} $ with distribution of X from equation \eqref{pdf} is
\begin{align}
    Var\brak{\dfrac{1}{\Bar{X}}} &= \dfrac{n^2\lambda^2}{(3n-1)^2(3n-2)}
\end{align}
\end{lemma}
\begin{proof}
 Similar to lemma \ref{expected} we have, rv $ T$
 \begin{align}
   Var\brak{\dfrac{1}{\Bar{X}}}    &=  \brak{ E \left[   {\dfrac{1}{\Bar{X}}}^2  \right] - { E \left[   \dfrac{1}{\Bar{X}}  \right]}^2}
 \end{align}
 To calculate, $E \left[   {\dfrac{1}{\Bar{X}}}^2  \right] $ we use,
 \begin{align}
      {\dfrac{1}{\Bar{X}}}^2 = \dfrac{n^2}{t^2}
 \end{align}
 \begin{align}
    &  E \left[   {\dfrac{1}{\Bar{X}}}^2  \right] = \int_{0}^{\infty} \dfrac{n^2}{t^2} \dfrac{1}{\Gamma(3n)} \lambda^{3n}t^{3n-1}e^{-\lambda t}\,dt \\
      &= \dfrac{n^2\lambda^2}{(3n-1)(3n-2)} \times (1)
 \end{align}
 As from lemma \ref{gp} we have,
 \begin{align}
     \int_{0}^{\infty} \dfrac{1}{\Gamma(3n-2)} \lambda^{3n-2}t^{3n-3}e^{-\lambda t}\,dt = 1
 \end{align}
Therefore,
\begin{align}
    Var\brak{\dfrac{1}{\Bar{X}}} &= \brak{\dfrac{n^2\lambda^2}{(3n-1)(3n-2)} - \dfrac{n^2\lambda^2}{(3n-1)^2}} \\
    &= \dfrac{n^2\lambda^2}{3n-1} \brak{\dfrac{1}{3n-2}- \dfrac{1}{3n-1}} \\
    &= \dfrac{n^2\lambda^2}{(3n-1)^2(3n-2)}
\end{align}
\end{proof}
\textbf{Solving all options : }
\begin{enumerate}
    \item 
  Now here we have our estimator $ \Theta$ and estimand $ \theta $ as,
 \begin{align}
     \Theta = \dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} \text{  and  }
     \theta = \lambda
 \end{align}
The expectation value of the estimator is given by, 
\begin{align}
    E[\Theta ] &= E  \left[   \dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i}  \right] \\
    & = \dfrac{2}{n} \sum_{i=1}^{n} E  \left[ \dfrac{1}{X_i}  \right] \\
 \end{align}
 Substituting $ n=1 $ in lemma \ref{expected},
 \begin{align}
   E \left[  \dfrac{1}{X_i}  \right] &= \dfrac{1 \times\lambda}{3-1}
  &= \dfrac{\lambda}{2} \\
 E[\Theta]     &= \dfrac{2n}{n} \times \dfrac{\lambda}{2} \\
              &= \lambda
\end{align}
So the bias of estimator is given by,
\begin{align}
    B(\Theta) &= E[\Theta] - \theta  \\
    &= \lambda - \lambda = 0
\end{align}
Therefore $\dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} $ is an unbiased estimator of $ \lambda$ \\
Option 1 is correct. \\
\item
 Now in this option we have our estimator $ \Theta$ and quantity to be estimated $ \theta $ as,
 \begin{align}
     \Theta = \dfrac{3n}{\sum_{i=1}^{n} X_i } \text{  and  }
     \theta = \lambda
 \end{align}
We have that sample mean, $ \Bar{X}$,
\begin{align}
    \Bar{X} &= \dfrac{X_1+X_2+ \cdots +X_n}{n} \\
    & = \dfrac{\sum_{i=1}^{n} X_i}{n}
\end{align}
Therefore estimator,
\begin{align}
    \Theta &= \dfrac{3n}{n \Bar{X}} 
    = \dfrac{3}{ \Bar{X}} 
    \label{xbar}
\end{align}
To check the distribution is gamma or not we calculate, MGF , i.e., $ M_X(s) $ for random variable, $ X $

\begin{align}
    M_X(s) &= E[e^{sX}]
    \end{align}
Using theorem \ref{gamma} and lemma \ref{ibp} we have,
\begin{align}
    M_X(s) & = \dfrac{\lambda^3}{2} \times \dfrac{\Gamma(3)}{(\lambda- s)^3} \\
    &= \brak{\dfrac{\lambda}{\lambda-s}}^3  \text{, as $\Gamma(3) = 2!$} \\
    &= \brak{\dfrac{\lambda}{\lambda-s}}^\alpha  \text{, for $\alpha= 3$} 
\end{align}
Therefore the MGF of $ X $ is same as the MGF of gamma distribution. \\
So from the theorem \ref{mgf}, the distribution of $ X$ is gamma distribution, i.e. $ X \sim \Gamma(\alpha,\lambda)$ with PDF,
\begin{align}
f_{X}(x)  = 
\begin{cases}
\dfrac{\lambda^{\alpha}x^{\alpha-1}e^{-\lambda x}}{\Gamma(\alpha)} &  x > 0
\\
0 & otherwise
\end{cases}
\end{align}
where $ \alpha = 3 $. \\
So we have from lemma \ref{expected} and equation \eqref{xbar},
\begin{align}
    E[\Theta] &= E\left[  \dfrac{3}{\Bar{X}}  \right]  \\
    &= 3\times  E \left[  \dfrac{1}{\Bar{X}}  \right] \\
    &= \dfrac{3n\lambda}{3n-1}
\end{align}
So we calculate bias as follows,
\begin{align}
    B(\Theta) & = E[\Theta] - \lambda \\
    &= \dfrac{3n\lambda}{3n-1} - \lambda \\
    &= \dfrac{\lambda}{3n-1} \neq 0
\end{align}
Therefore $ \dfrac{3n}{\sum_{i=1}^{n} X_i } $ is not an unbiased estimator of $ \lambda$ \\
Option 2 is not correct. \\
\item
 Now here we have our estimator $ \Theta$ and quantity to be estimated $ \theta $ as,
 \begin{align}
     \Theta = \dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} \text{  and  }
     \theta = \lambda
 \end{align}
Now the variance of $ \Theta$ is calculated using lemma \ref{var} as,
\begin{align}
    Var(\Theta) &= Var\brak{\dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} } \\
    & = \dfrac{4}{n^2} \sum_{i=1}^{n} Var\brak{\dfrac{1}{X_i}} 
    \label{eq3}
\end{align}
Now substituting $ n=1 $ in lemma \ref{varxbar} we have,
\begin{align}
     Var\brak{\dfrac{1}{X_i}} &= \dfrac{1^2\lambda^2}{(3-1)^2(3-2)}\\
     &= \dfrac{\lambda}{4}
\end{align}
Therefore,
\begin{align}
 Var(\Theta)   & = \dfrac{4n}{n^2} \times \dfrac{\lambda^2}{4} \\
    &= \dfrac{\lambda^2}{n}
\end{align}
The bias of $ \Theta $ from option 1 is given as
\begin{align}
    B(\Theta) = 0
\end{align}
So we have,
\begin{align}
    MSE(\Theta_n) &= Var(\Theta) + B(\Theta)^2 \\
    &= \dfrac{\lambda^2}{n}
\end{align}
Now,

\begin{align}
     \lim_{n\to\infty} MSE( \Theta_n) &=    \lim_{n\to\infty} \dfrac{\lambda^2}{n} \\
      &= 0
\end{align}
Therefore, $\dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} $ is a consistent estimator of $ \lambda$. 
Option 3 is correct. \\
\item 
 Now in this option we have our estimator $ \Theta$ and quantity to be estimated $ \theta $ as,
 \begin{align}
     \Theta = \dfrac{3n}{\sum_{i=1}^{n} X_i } \text{  and  }
     \theta = \lambda
 \end{align}
 The variance of estimator similar to option 2 and using lemma \ref{varxbar} is,
 \begin{align}
     Var(\Theta) &= Var\brak{\dfrac{3}{\Bar{X}}}\\
     &= 9 \times Var\brak{\dfrac{1}{\Bar{X}}}\\
     &=  \dfrac{9n^2\lambda^2}{(3n-1)^2(3n-2)}
 \end{align}
The bias calculated from option 2 is 
\begin{align}
    B(\Theta) = \dfrac{\lambda}{3n-1}
\end{align}
So we have, 
\begin{align}
    MSE(\Theta) &= Var(\Theta) + B(\Theta)^2 \\
    &= \dfrac{9n^2\lambda^2}{(3n-1)^2(3n-2)} + \dfrac{\lambda^2}{(3n-1)^2}
\end{align}
\begin{align}
    & \lim_{n\to\infty} MSE( \Theta_n) \\
     &= \lim_{n\to\infty} \dfrac{9n^2\lambda^2}{(3n-1)^2(3n-2)} + \dfrac{\lambda^2}{(3n-1)^2} 
\end{align}
Now in first limit multiply and divide by $ n^2$ and $ {n\to\infty} $ we get,
\begin{align}
     & \lim_{n\to\infty} MSE( \Theta_n) =0
\end{align}
Therefore, $\dfrac{3n}{\sum_{i=1}^{n} X_i} $ is a consistent estimator of $ \lambda$. 
Option 4 is correct. \\
\end{enumerate}
\textbf{Therefore option 1, option 3 and option 4 are correct.}
\end{document}