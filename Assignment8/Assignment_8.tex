\documentclass[journal,12pt,twocolumn]{IEEEtran}

\usepackage{setspace}
\usepackage{gensymb}
\singlespacing
\usepackage[cmex10]{amsmath}

\usepackage{amsthm}

\usepackage{mathrsfs}
\usepackage{txfonts}
\usepackage{stfloats}
\usepackage{bm}
\usepackage{cite}
\usepackage{cases}
\usepackage{subfig}

\usepackage{longtable}
\usepackage{multirow}

\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{steinmetz}
\usepackage{tikz}
\usepackage{circuitikz}
\usepackage{verbatim}
\usepackage{tfrupee}
\usepackage[breaklinks=true]{hyperref}
\usepackage{graphicx}
\usepackage{tkz-euclide}

\usetikzlibrary{calc,math}
\usepackage{listings}
    \usepackage{color}                                            %%
    \usepackage{array}                                            %%
    \usepackage{longtable}                                        %%
    \usepackage{calc}                                             %%
    \usepackage{multirow}                                         %%
    \usepackage{hhline}                                           %%
    \usepackage{ifthen}                                           %%
    \usepackage{lscape}     
\usepackage{multicol}
\usepackage{chngcntr}

\DeclareMathOperator*{\Res}{Res}

\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}

\renewcommand\thesectiondis{\arabic{section}}
\renewcommand\thesubsectiondis{\thesectiondis.\arabic{subsection}}
\renewcommand\thesubsubsectiondis{\thesubsectiondis.\arabic{subsubsection}}


\hyphenation{op-tical net-works semi-conduc-tor}
\def\inputGnumericTable{}                                 %%

\lstset{
%language=C,
frame=single, 
breaklines=true,
columns=fullflexible
}
\begin{document}
\newcommand{\Int}{\int\limits}
\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}
\bibliographystyle{IEEEtran}
\raggedbottom
\setlength{\parindent}{0pt}
\providecommand{\mbf}{\mathbf}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\providecommand{\abs}[1]{\vert#1\vert}
\providecommand{\res}[1]{\Res\displaylimits_{#1}} 
\providecommand{\norm}[1]{\lVert#1\rVert}
%\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\mtx}[1]{\mathbf{#1}}
\providecommand{\mean}[1]{E[ #1 ]}
\providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
%\providecommand{\hilbert}{\overset{\mathcal{H}}{ \rightleftharpoons}}
\providecommand{\system}{\overset{\mathcal{H}}{ \longleftrightarrow}}
	%\newcommand{\solution}[2]{\textbf{Solution:}{#1}}
\newcommand{\solution}{\noindent \textbf{Solution: }}
\newcommand{\cosec}{\,\text{cosec}\,}
\providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
\newcommand{\myvec}[1]{\ensuremath{\begin{pmatrix}#1\end{pmatrix}}}
\newcommand{\mydet}[1]{\ensuremath{\begin{vmatrix}#1\end{vmatrix}}}
\numberwithin{equation}{subsection}
\makeatletter
\@addtoreset{figure}{problem}
\makeatother
\let\StandardTheFigure\thefigure
\let\vec\mathbf
\renewcommand{\thefigure}{\theproblem}
\def\putbox#1#2#3{\makebox[0in][l]{\makebox[#1][l]{}\raisebox{\baselineskip}[0in][0in]{\raisebox{#2}[0in][0in]{#3}}}}
     \def\rightbox#1{\makebox[0in][r]{#1}}
     \def\centbox#1{\makebox[0in]{#1}}
     \def\topbox#1{\raisebox{-\baselineskip}[0in][0in]{#1}}
     \def\midbox#1{\raisebox{-0.5\baselineskip}[0in][0in]{#1}}
\vspace{3cm}
\title{AI1103-Assignment 8}
\author{Name : Ayush Jha \\ Roll Number: CS20BTECH11006}
\maketitle
\newpage
\bigskip
\renewcommand{\thefigure}{\theenumi}
\renewcommand{\thetable}{\theenumi}
Download all latex-tikz codes from 
%
\begin{lstlisting}
https://github.com/ayushjha2612/AI11003/tree/main/Assignment8
\end{lstlisting}

\section*{CSIR UGC NET EXAM (June 2013) Q. 68}
Let $ X_1, \cdots , X_n $ be independent and identically distributed random variables with probability density function
\begin{align*}
    f(x) = \frac{1}{2} \lambda^3x^2e^{-\lambda x} ; x>0 ; \lambda > 0
\end{align*}
Then which of the following statements are true?
\begin{enumerate}
    \item $\dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} $ is an unbiased estimator of $ \lambda$
    \item $\dfrac{3n}{\sum_{i=1}^{n} X_i } $ is an unbiased estimator of $ \lambda$ \\
    \item $\dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} $ is a consistent estimator of $ \lambda$
    \item $\dfrac{3n}{\sum_{i=1}^{n} X_i } $ is a consistent estimator of $ \lambda$
\end{enumerate}

\section*{Solution}
\textbf{Solving all options : }
\begin{enumerate}
    \item 
    \textbf{Definition Unbiased Estimator : } \\ Let $\Theta = h(X_1,X_2, \cdots , X_n) $ be a point estimator for a parameter $ \theta $. We say that $ \Theta $ is an unbiased of estimator of $ \theta $ if
    \begin{align}
       B(\Theta )= 0 \text{, for all possible values of $\theta$.}
    \end{align}
Where the bias of the estimator $ \Theta $ is defined by 
\begin{align}
    B(\Theta ) = E[\Theta ] - \theta
\end{align}
 Now here we have our estimator $ \Theta$ and quantity to be estimated $ \theta $ as,
 \begin{align}
     \Theta = \dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} \text{  and  }
     \theta = \lambda
 \end{align}

The expectation value of the estimator is given by, 
\begin{align}
    E[\Theta ] &= E  \left[   \dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i}  \right] \\
    & = \dfrac{2}{n} \sum_{i=1}^{n} E  \left[ \dfrac{1}{X_i}  \right] \\
    & =  \dfrac{2}{n} \sum_{i=1}^{n} \int_{-\infty}^{\infty} \dfrac{1}{x} f(x)\,dx \\
    \label{eq1}
    & = \dfrac{2n}{n} \int_{0}^{\infty} \dfrac{1}{x} \dfrac{1}{2} \lambda^3x^2e^{-\lambda x}\,dx \\
    & = \lambda^3  \int_{0}^{\infty}  x e^{-\lambda x}\,dx \\
    & = \lambda^3   \brak{ \dfrac{-x e^{-\lambda x}}{\lambda}- \dfrac{x e^{-\lambda x}}{\lambda^2}} \Biggr|_{0}^{\infty} \\
    & = \lambda^3 \brak{-  \left[ 0 -   \dfrac{1}{\lambda^2}  \right]} \\
    &= \lambda
\end{align}
So the bias of estimator is given by,
\begin{align}
    B(\Theta) &= E[\Theta] - \theta  \\
    &= \lambda - \lambda = 0
\end{align}
Therefore $\dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} $ is an unbiased estimator of $ \lambda$ \\
Option 1 is correct. \\
\item
 Now in this option we have our estimator $ \Theta$ and quantity to be estimated $ \theta $ as,
 \begin{align}
     \Theta = \dfrac{3n}{\sum_{i=1}^{n} X_i } \text{  and  }
     \theta = \lambda
 \end{align}

The expectation value of the estimator is given by, 
\begin{align}
    E[\Theta ] &= E  \left[   \dfrac{3n}{\sum_{i=1}^{n} X_i }  \right] \\
    & = \dfrac{3n}{\sum_{i=1}^{n}}  E  \left[ \dfrac{1}{X_i}  \right] 
\end{align}
The value of $  E  \left[ \dfrac{1}{X_i}  \right]  $ can be obtained from \eqref{eq1} as 
\begin{align} 
     E  \left[ \dfrac{1}{X_i}  \right] = \dfrac{\lambda}{2}
     \label{eq2}
\end{align}
So we have,
\begin{align}
     E[\Theta ] &= \dfrac{3n}{\sum_{i=1}^{n}} \dfrac{\lambda}{2} \\
     &= \dfrac{3n}{n} \dfrac{\lambda}{2} \\
     &= \dfrac{3\lambda}{2}
\end{align}
So the bias of estimator is given by,
\begin{align}
    B(\Theta) &= E[\Theta] - \theta  \\
    &= \dfrac{3\lambda}{2}- \lambda \\
    &= \dfrac{\lambda}{2} \neq 0
\end{align}
Therefore $ \dfrac{3n}{\sum_{i=1}^{n} X_i } $ is not an unbiased estimator of $ \lambda$ \\
Option 2 is not correct. \\
\item
\textbf{Definition Consistent Estimator : } \\
Let $ \Theta_1,\Theta_2, \cdots, \Theta_n , \cdots, $  be a sequence of point estimators of $ \theta $. We say that $ \Theta_n $ is a \textbf{consistent} estimator of $ \theta $, if 
\begin{align}
    \lim_{n\to\infty} \Pr\brak{| \Theta_n - \theta | \ge \epsilon} = 0 \text{ ,for all $ \epsilon > 0$.}
\end{align}
We can equivalently say that 
\begin{align}
     \lim_{n\to\infty} MSE( \Theta_n) = 0,
\end{align}
then $ \Theta_n $ is a consistent estimator of $ \theta$. \\
The mean squared error (MSE) of a point estimator $ \Theta $, shown by $ MSE(\Theta) $, is defined as
\begin{align}
    MSE(\Theta ) &= E[(\Theta - \theta)^2] \\
    &= Var(\Theta) + B(\Theta)^2
\end{align}
where $ B(\Theta ) $ is the bias of $ \Theta $. \\
 Now here we have our estimator $ \Theta$ and quantity to be estimated $ \theta $ as,
 \begin{align}
     \Theta = \dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} \text{  and  }
     \theta = \lambda
 \end{align}
Now the variance of $ \Theta$ is calculated as
\begin{align}
    Var(\Theta) &= Var\brak{\dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} } \\
    & = \dfrac{4}{n^2} \sum_{i=1}^{n} Var\brak{\dfrac{1}{X_i}} \\
    \label{eq3}
    & = \dfrac{4n}{n^2} \brak{E  \left[ {\dfrac{1}{X_i}}^2  \right] - {E  \left[ {\dfrac{1}{X_i}}  \right]}^2 } \\
    & = \dfrac{4}{n} \brak{ \int_{-\infty}^{\infty} \dfrac{1}{x^2} f(x)\,dx  - \brak{\dfrac{\lambda}{2}}^2 } \\
    & =  \dfrac{4}{n} \brak{ \int_{0}^{\infty} \dfrac{1}{x^2}  \dfrac{1}{2} \lambda^3x^2e^{-\lambda x} \,dx  - {\dfrac{\lambda^2}{4}} } \\
      & =  \dfrac{4}{n} \brak{ \dfrac{\lambda^3}{2}\int_{0}^{\infty} e^{-\lambda x} \,dx  - {\dfrac{\lambda^2}{4}} } \\ 
    & =  \dfrac{4}{n} \brak{ {\dfrac{\lambda^2}{2}} - {\dfrac{\lambda^2}{4}} } \\ 
    &= \dfrac{\lambda^2}{n}
\end{align}
The bias of $ \Theta $ from option 1 is given as
\begin{align}
    B(\Theta) = 0
\end{align}
So we have,
\begin{align}
    MSE(\Theta_n) &= Var(\Theta) + B(\Theta)^2 \\
    &= \dfrac{\lambda^2}{n}
\end{align}
Now,
\begin{align}
     \lim_{n\to\infty} MSE( \Theta_n) &=    \lim_{n\to\infty} \dfrac{\lambda^2}{n} \\
      &= 0
\end{align}
Therefore, $\dfrac{2}{n} \sum_{i=1}^{n} \dfrac{1}{X_i} $ is a consistent estimator of $ \lambda$. 
Option 3 is correct. \\
\item 
 Now in this option we have our estimator $ \Theta$ and quantity to be estimated $ \theta $ as,
 \begin{align}
     \Theta = \dfrac{3n}{\sum_{i=1}^{n} X_i } \text{  and  }
     \theta = \lambda
 \end{align}
Now the variance of $ \Theta$ is calculated as
\begin{align}
    Var(\Theta) &= Var\brak{\dfrac{3n}{\sum_{i=1}^{n} X_i } } \\
    & = \dfrac{9n^2}{ \sum_{i=1}^{n}} Var\brak{\dfrac{1}{X_i}} \\
\end{align}
Now the value of $ Var\brak{\dfrac{1}{X_i}} $ from \eqref{eq3} is substituted, we have
\begin{align}
     Var(\Theta) &= \dfrac{9n^2}{ \sum_{i=1}^{n}} \dfrac{\lambda^2}{4} \\
     & = \dfrac{9n^2 \lambda^2}{4n }
     & = \dfrac{9n \lambda^2}{4 }
\end{align}
The bias of $ \Theta $ from option 2 is given as
\begin{align}
    B(\Theta) = \dfrac{\lambda}{2}
\end{align}
So we have,
\begin{align}
    MSE(\Theta_n) &= Var(\Theta) + B(\Theta)^2 \\
    &= \dfrac{9n \lambda^2}{4 } + \brak{\dfrac{\lambda}{2}}^2 \\
    & = \dfrac{ \lambda^2}{4 } (9n+1)
\end{align}
Now,
\begin{align}
     \lim_{n\to\infty} MSE( \Theta_n) &=    \lim_{n\to\infty} \dfrac{ \lambda^2}{4 } (9n+1) \\
\end{align}
Clearly as n grows larger $ 9n+1$ also grows larger, so
\begin{align}
     \lim_{n\to\infty} MSE( \Theta_n) \neq 0   
\end{align}
Therefore, $\dfrac{3n}{\sum_{i=1}^{n} X_i} $ is not a consistent estimator of $ \lambda$. \\
Option 4 is not correct. \\
\end{enumerate}
\textbf{Therefore option 1 and option 3 are correct.}
\end{document}